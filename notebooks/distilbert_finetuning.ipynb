{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd \nimport re\nimport torch\nimport warnings\nimport logging\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\n\n!pip install ipywidgets==8.1.5\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category = UserWarning)\n\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\nfor logger in loggers:\n    if \"transformers\" in logger.name.lower():\n        logger.setLevel(logging.ERROR)\n\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nkaggle_test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint(\"\\nBelow is the training data:\\n\")\ntrain_df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:58:12.080607Z","iopub.execute_input":"2025-07-14T13:58:12.081473Z","iopub.status.idle":"2025-07-14T13:58:19.046157Z","shell.execute_reply.started":"2025-07-14T13:58:12.081399Z","shell.execute_reply":"2025-07-14T13:58:19.045148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nData pre-processing\n'''\nhashtag_count = 0\nurl_count = 0\nmentions_count = 0\n\nfor tweet in train_df['text']:\n    \n    if re.search(r'#\\w+', tweet):\n        hashtag_count += 1\n    \n   \n    if re.search(r'http\\S+', tweet):\n        url_count += 1\n        \n    if re.search(r'@\\w+', tweet):\n        mentions_count += 1\n\nprint(\"\\n{} tweets in the training data contain hashtag\".format(hashtag_count))\nprint(\"\\n{} tweets in the training data contain URL\".format(url_count))\nprint(\"\\n{} tweets in the training data contain mentions\".format(mentions_count))\n\nprocessed_train_df = train_df.copy()\n\ndef clean_text(text):\n    text = re.sub(r\"@\\w+\", \"\", text)     # Remove @mentions\n    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n    text = re.sub(r\"<.*?>\", \"\", text)    # Remove HTML tags\n    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)  # Remove special chars\n    text = re.sub(r\"\\s+\", \" \", text)     # Remove extra whitespace\n    return text.strip().lower()\n\nprocessed_train_df[\"keyword\"] = processed_train_df[\"keyword\"].fillna(\"\")\nprocessed_train_df[\"text\"] = processed_train_df[\"keyword\"] + \" \" + processed_train_df[\"text\"]\nprocessed_train_df[\"text\"] = processed_train_df[\"text\"].apply(clean_text)\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(processed_train_df[\"text\"].tolist(),\n                                                                    processed_train_df[\"target\"].tolist(),\n                                                                    test_size=0.1,\n                                                                    random_state=42)\n\n#Plotting the class distribution in the target\nplt.figure(figsize=(6, 4))\nprocessed_train_df['target'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n\nplt.title('Count of 1s and 0s in the \"disaster\" Column')\nplt.xlabel('Disaster Label (0 = Not Disaster, 1 = Disaster)')\nplt.ylabel('Count')\n\nplt.xticks([0, 1], ['Not Disaster (0)', 'Disaster (1)'])  # Label x-axis\nplt.show()\n\n#Plotting the distribution of tweet length\nplt.figure(figsize=(6, 4))\nprocessed_train_df['tweet_length'] = processed_train_df['text'].apply(len)\n\nsns.histplot(processed_train_df['tweet_length'], binwidth=5, kde=True)\nplt.title('Tweet Length Distribution')\nplt.xlabel('Tweet Length (characters)')\nplt.ylabel('Frequency')\nplt.show()\n\n#Plotting word count distribution of tweets\nplt.figure(figsize=(6, 4))\nprocessed_train_df['word_count'] = processed_train_df['text'].apply(lambda x: len(x.split()))\n\nsns.histplot(processed_train_df['word_count'], discrete=True, kde=True)\nplt.title('Word Count Distribution')\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.show()\n\nprocessed_train_df.drop(columns = ['tweet_length','word_count'], inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T13:58:26.739056Z","iopub.execute_input":"2025-07-14T13:58:26.739424Z","iopub.status.idle":"2025-07-14T13:58:27.791185Z","shell.execute_reply.started":"2025-07-14T13:58:26.739392Z","shell.execute_reply":"2025-07-14T13:58:27.790341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nFine-tuning a distilbert transformer model to predict disaster\n'''\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)\n\ntrain_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], train_labels)\nval_dataset = TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"], val_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\n# ---------- Model, Optimizer, Scheduler ----------\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\nnum_training_steps = len(train_loader) * 10  # max epochs (early stopping may stop earlier)\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# ---------- Training and Evaluation ----------\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    loop = tqdm(data_loader, desc=\"Training\")\n    for batch in loop:\n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n        loop.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(data_loader)\n    acc = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    return avg_loss, acc, f1\n\ndef eval_model(model, data_loader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    loop = tqdm(data_loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for batch in loop:\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = sum([p == l for p, l in zip(all_preds, all_labels)]) / len(all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    return acc, f1\n\n# ---------- Training Loop with Early Stopping ----------\nEPOCHS = 10\nbest_f1 = 0\npatience = 2\nwait = 0\n\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n    \n    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, lr_scheduler, device)\n    val_acc, val_f1 = eval_model(model, val_loader, device)\n\n    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train F1: {train_f1:.4f}\")\n    print(f\"Val Acc  : {val_acc:.4f} | Val F1  : {val_f1:.4f}\")\n\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        wait = 0\n        model.save_pretrained(\"distilbert-disaster-model\")\n        tokenizer.save_pretrained(\"distilbert-disaster-model\")\n        print(\"✅ Model improved — saved to 'distilbert-disaster-model'\")\n    else:\n        wait += 1\n        print(f\"⚠️  No improvement. Early stopping wait: {wait}/{patience}\")\n        if wait >= patience:\n            print(\"⏹️  Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:16:30.080481Z","iopub.execute_input":"2025-07-14T14:16:30.081223Z","iopub.status.idle":"2025-07-14T14:19:27.794416Z","shell.execute_reply.started":"2025-07-14T14:16:30.081181Z","shell.execute_reply":"2025-07-14T14:19:27.793513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nPredicting if a tweet is about a diasater on kaggle test dataset \n'''\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"./distilbert-disaster-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"./distilbert-disaster-model\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Tokenize the test tweets\nkaggle_test_encodings = tokenizer(kaggle_test_df['text'].tolist(),\n                                  padding=True,\n                                  truncation=True,\n                                  max_length=160,\n                                  return_tensors=\"pt\")\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Prepare input data\ninput_ids = kaggle_test_encodings['input_ids'].to(device)\nattention_mask = kaggle_test_encodings['attention_mask'].to(device)\n\n\n# Make predictions\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n\n# Convert predictions to list\npredicted_labels = predictions.cpu().numpy()\n\nsubmission_df = pd.DataFrame({'id': kaggle_test_df['id'],\n                              'target': predicted_labels})\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nBelow is the submission:\")\nprint(submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T14:21:19.797005Z","iopub.execute_input":"2025-07-14T14:21:19.797370Z","iopub.status.idle":"2025-07-14T14:21:27.578222Z","shell.execute_reply.started":"2025-07-14T14:21:19.797328Z","shell.execute_reply":"2025-07-14T14:21:27.577271Z"}},"outputs":[],"execution_count":null}]}